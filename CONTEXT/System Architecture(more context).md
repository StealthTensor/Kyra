System Architecture
The system follows a modular, multi-layer architecture. At the top is a mobile-first UI (e.g. React Native or a PWA in Next.js) that handles multiple Gmail account OAuth logins and displays email threads. The UI communicates with a backend API that orchestrates Gmail API calls, calendar integrations, and LLM-powered features. An API/service layer (Node.js or Python/Flask/FastAPI) handles requests, authenticates with Google OAuth, and exposes endpoints (or GraphQL/tRPC) for the front end. A task/scheduler layer (e.g. Celery/Redis or Node “bull” queues) runs periodic jobs (e.g. inbox digests, email prioritization, summarization).
The data layer includes:
- Primary database (PostgreSQL or similar) storing user profiles (preferences, OAuth tokens), email metadata (message ID, thread ID, subject, snippet, labels, timestamps, sender/recipient, attachments list, read/important flags), and conversation/chat history. Conversations and message logs can be stored relationally (e.g. tables for users, threads, messages)[1].
- Vector store (e.g. Pinecone, Weaviate, Qdrant, or PostgreSQL+pgvector) holding embeddings of emails, attachments, and relevant memory (for semantic search, retrieval, and context)[2][3]. This enables RAG-style lookups and “memory” of past emails.
The LLM inference layer is abstracted behind a router/adapter. This layer can invoke different providers (OpenAI GPT, Google Gemini, Groq LPU, etc.) via a common interface. Tools like LangChain (Python) or equivalents in Node can manage multi-provider calls; as one example notes, LangChain offers a framework-agnostic abstraction for multiple LLMs (Groq, OpenAI, Anthropic, etc.)[4]. In practice, the backend will use an “LLM client” interface (or a LangChain pipeline) to route requests (classification, summarization, composing, etc.) to the chosen model API or on-prem engine. The architecture allows easy swapping of LLMs or using fallbacks.
Google integrations: Separate service modules interface with the Gmail API (via OAuth) and Google Calendar API. Email actions (fetch, send, label updates, auto-replies) go through Gmail REST endpoints. Calendar events and tasks use Google Calendar API. These modules update the primary DB (e.g. storing new messages, syncing calendars) and trigger LLM pipelines as needed (e.g. “schedule meeting” from a chat command).
Real-time sync across devices is achieved with a publish/subscribe or WebSocket layer. For example, a realtime service (e.g. Supabase Realtime or a self-hosted socket server) broadcasts updates when new emails arrive or actions complete. Client apps subscribe to updates so that inbox and chat views stay synchronized on mobile and web[5].
Overall data flow: user or scheduled events → backend fetches Gmail data → preprocess (parse text, attachments) → store metadata in DB + compute embeddings in vector store → update user-visible inbox (real-time push). In chat, user prompts are sent to the backend, which assembles conversation history (from DB), runs retrieval (vector DB), calls the LLM pipeline to compose a response or action, then returns it to UI (and possibly performs actions like “send email” via Gmail API)[6][4].
Data & Memory Schemas
•	Users / Profiles: Store each Gmail account with fields like user_id, email_address, OAuth access_token/refresh_token, token expiry, and user preferences (e.g. default sort order, summary frequency, signature, notification settings). Also store a list of authorized Gmail accounts per user. This can use a relational table or a managed auth service (e.g. Supabase Auth)[1].
•	Email Metadata: Each email message is an entry with fields: message_id, thread_id, account_id (which Gmail account it belongs to), from, to, cc, subject, timestamp, snippet/body, labels/tags (e.g. Inbox, Starred), has_attachments, priority_score, etc. For example, a PostgreSQL table emails might have columns for mail_id, subject, snippet, fetched_at, etc. (a sample schema is shown in [16] for raw Gmail data)[7]. Metadata also includes processing flags (e.g. is_processed, is_important, or categories assigned by the AI).
•	Attachment Data: Attachments (PDFs, docs) are stored separately (possibly in object storage). The database holds attachment metadata (attachment_id, email_id, filename, mime_type, s3_key). Attachment content is fetched on demand and fed into an OCR/text-extraction pipeline, and its extracted text may be stored or embedded.
•	Conversation Memory: For the chat interface, maintain conversation transcripts. A simple schema: conversations(conversation_id, user_id, created_at) and messages(message_id, conversation_id, role (user/assistant), text, timestamp). This allows reconstructing the chat context. Optionally, a memory buffer or summary (short-term memory) table can store compressed context or highlights for long chats, following LangChain patterns (ConversationBufferMemory, etc.)[8].
•	Vector Store Schema: The vector DB (e.g. Pinecone index, Weaviate collection, or pgvector table) stores embeddings with metadata pointers. Each vector record includes an id (which can be the email or paragraph ID) and a metadata payload (e.g. email_id, type, timestamp, maybe summary). For example, store embeddings of each email body or each sentence. Querying retrieves similar emails or past chat turns. As shown in other designs, a vector “memory layer” can index all historical emails/summaries[2][3].
•	Preferences & Settings: A table preferences(user_id, key, value) can store learning data (e.g. “I flagged sender X as important”) or UI preferences (theme, notifications). Learned importance might also update fields in the emails table (e.g. priority score) or a separate email_priority(user_id, sender_email, score) table.
Each component (mail fetcher, classifier, summarizer) logs results to the DB for persistence and audit. For example, storing the summary or categorization of each email thread can help the AI improve over time.
Technology Stack Recommendations
•	Frontend (UI): Use a modern React-based stack. For mobile-first and cross-platform support, React Native (with Expo) is a strong choice, enabling the same codebase on iOS/Android. Alternatively, a web-first PWA using Next.js or Vite+React can provide a rich interface with responsiveness. In either case, utilize a contemporary UI toolkit (e.g. Tailwind CSS or Chakra UI for web, or React Native Paper/Dripsy for mobile) to ensure a sleek, responsive design. Open-source component libraries like shadcn/ui (built on Radix + Tailwind) or Chakra/Ant Design can accelerate development[9][10]. Key front-end libraries include React Query (for server state and caching) and React Navigation (for RN). For type safety and developer experience, use TypeScript throughout as in popular stacks[11].
•	Backend/API Server: Use a Node.js/TypeScript backend (e.g. with Express, NestJS, or tRPC) or Python with FastAPI, depending on team expertise. Node is natural for a React ecosystem; FastAPI is excellent for async tasks and has solid LLM and async job libraries. The backend will expose a REST/GraphQL/tRPC API for client operations (login, fetch emails, send commands). It will also host OAuth endpoints to handle Gmail/Calendar token refresh. Follow patterns like the VectorMail example (Next.js + tRPC + Postgres)[12][11].
•	LLM Routing Layer: Implement an abstraction so the system can use any LLM provider. For example, use LangChain (JS or Python) to manage prompts and tool usage. LangChain supports custom LLM classes and can switch models via configuration[4]. Alternatively, use an internal strategy: define an LLMService interface with methods for classify, summarize, chat, etc., and implement it for each provider (OpenAI GPT, Google Gemini via API, Groq via API, etc.). You might also consider services like CrewAI, LangGraph, or agent frameworks if needing complex orchestration, but a simpler pipeline may suffice for MVP.
•	Vector Database: For memory and semantic search, use a hosted vector DB. Pinecone and Weaviate are popular choices (scalable, API-driven)[2]. Qdrant or Redis Vector (open-source) are also options. If using PostgreSQL, pgvector extension can store vectors alongside metadata (as seen in VectorMail)[13]. Choose based on scale: Pinecone/Weaviate for simplicity, or a self-hosted Qdrant if open-source is preferred.
•	Primary Database: Use PostgreSQL (or another RDBMS) for structured data (users, emails, settings). Many systems use Postgres with Prisma or SQLAlchemy ORM for reliability[13][1]. Redis can serve as a cache or transient store for sessions and quick lookups. (For example, use Redis for rate-limiting or caching Google API tokens.)
•	Real-Time Updates: To push inbox updates and chat messages instantly, use a real-time pub/sub. Options:
•	WebSockets via a service like Socket.IO or built-in broadcasting (if using Next.js, the Realtime API, or GraphQL subscriptions).
•	Supabase Realtime: As documented, Supabase can broadcast low-latency messages between clients (perfect for chat/inbox updates)[5].
•	Firebase: Real-time DB or Cloud Messaging for push notifications.
This layer publishes events (new email fetched, chat reply ready, calendar event updated) which client apps subscribe to.
•	Scheduler/Background Jobs: Use Celery (Python) or BullMQ (Node) with Redis to run background tasks: periodically fetch new emails, rerank inbox, generate daily digests, or process attachments. The dev example uses Celery/Redis for orchestration[14]. Cron or serverless scheduled functions could also trigger certain routines.
•	Calendar Integration: Use Google Calendar API with OAuth. Store events in the database if needed for local operations. For tasks, either map them to Google Tasks (via API) or maintain a simple tasks table.
•	Security/Auth: Use OAuth 2.0 for Google login/permissions for Gmail and Calendar[1]. For user management, a system like Supabase Auth or Auth0 can simplify login (though here Google is primary login). Store tokens securely, e.g. in encrypted DB fields or a secrets manager. Use HTTPS everywhere and rotate tokens on expiry.
LLM Inference & Tool Layer
Design a LLM abstraction layer so the system can “plug in” different model providers. For example: - Define an interface with methods like classifyEmail(text), summarizeThread(threadText), composeReply(prompt), etc. - Implement this interface using different SDKs or APIs: e.g. OpenAI ChatCompletion (GPT-4), Google’s VertexAI API for Gemini, Perplexity API, or any Groq endpoint.
- Consider using LangChain or an agent framework: LangChain’s Chains and Tools allow combining LLM calls with structured tools (Gmail API calls, calendaring). As noted by architects, LangChain can unify multiple model providers behind one abstraction[4]. - Use tool/functional calling when possible: e.g., enable “function calls” in GPT-4o or use LangChain tool abstraction to let the LLM invoke the Gmail API (for send email) or Calendar API (for scheduling) by calling a defined function. This ensures the LLM doesn’t misuse user data and can only perform allowed actions.
In practice, your inference layer will: 1. Accept a query (user prompt or internal trigger). 2. Decide workflow: e.g. normal chat vs summary vs classification vs action. 3. Retrieve context: use vector DB to fetch relevant past emails/messages if needed (RAG). 4. Call LLM: send messages (system + history + user query) to chosen LLM model, maybe as a chat completion with function/tool support. For instance, to draft a reply, you might call GPT-4 with a system prompt about style. 5. Post-process: parse the response. If it includes a function call (e.g. “send_email” with parameters), execute it via your Gmail integration. Store generated outputs (e.g. the actual reply text, summary) in the database, and update state (sent status, etc.). 6. Fallbacks: If one model fails (rate-limit or error), automatically retry with another configured LLM (e.g. failover from one provider to another).
By isolating LLM calls in one module and parameterizing the model provider, you satisfy the “modular LLM backend” requirement. Logs of queries and responses should be stored (perhaps in the DB) for debugging and to train preference models.
Features Implementation Notes
•	Email Sorting/Prioritization: On each fetch, run an LLM classifier (e.g. GPT prompt) to assign an importance score or tag (e.g. “Urgent” vs “Bulk”). Combine AI output with simple heuristics (sender reputation, keywords) to rank inbox items. Store these tags and use them in UI sorting. Over time, learn from user actions: if a user moves certain emails to “important”, update a learning model (or vector embeddings for that sender) to boost priority. (The dev example shows a simple LLM urgency rating prompt[15].)
•	Summaries/Digests: Periodically (e.g. once daily or on-demand), batch emails and call the LLM to produce a short summary of the day’s communications or a long thread. For example, send each thread’s concatenated messages to GPT with a prompt like “Summarize the following email thread in 2 sentences”[16]. Store the summary in the DB (or send as a push notification/digest email).
•	Chatbox Interface: The front end provides a chat UI. User questions (“What’s my schedule next week?” or “Draft reply to John’s email”) are sent to the backend, which can either answer from cached data (e.g. upcoming Google Calendar events) or invoke the LLM. Maintains context by including recent messages in the prompt (or via the conversation memory DB). Optionally, implement conversation memory strategies: e.g. summarizing old chat turns once length grows, then replacing them with a shorter “memory” to stay within token limits.
•	Auto-Reply & Compose: When the user triggers an auto-reply, the system uses the LLM to generate a draft. For example, it pulls the latest email text and asks, “Write a polite reply as me.” The result is shown to the user for approval or directly sent, depending on settings. Then the backend calls Gmail’s “send” API. Keep a log of replies sent. Include an “undo send” cache (with Gmail’s short undo window if possible).
•	Attachment Processing: When an email with attachments arrives, the backend fetches attachments (e.g. via Gmail’s “getAttachment” API), uses OCR or a PDF parser to extract text, and then processes it. For example, chunk the text and feed it into the summarization pipeline (embedding + LLM summary). The summary is then attached to the email’s record. This lets the chatbox answer questions about attachments content. (This aligns with the goal “Attachment processing: summarize contents.”)
•	Cross-Device Sync: As noted, use WebSockets or a service like Supabase Realtime so that any action on one device (e.g. sending an email reply, marking important) immediately propagates to others. All state is centralized (via the backend DB), so any client can query the latest via the API and subscribe to updates[5]. For push notifications (e.g. new email arrived), integrate Firebase Cloud Messaging or a similar service.
Sources
•	Malok’s AI email assistant design (with Gmail/IMAP sync, LLM pipelines, vector memory)[17][2]
•	Plaban Nayak’s Gmail AI assistant (Streamlit/Arcade AI) for tech stack insights (React/TS UI, Python/FastAPI, Postgres, Supabase, LangChain)[1].
•	VectorMail (Next.js/T3 stack) for a layered architecture reference (Next.js+React UI, tRPC API, Prisma+Postgres+pgvector, multi-LLM support)[12][13].
•	Architecture guides on LLM assistants (LangChain abstraction for multi-models[4], Pinecone memory layer[2]) and realtime sync (Supabase Realtime docs[5]).
•	Various dev/blog posts on AI email agents (e.g. email classification and summarization with GPT[18][16]) which inspired the outlined pipelines.
________________________________________
[1] [6] ✨Building Building a Next-Generation Gmail AI Assistant with Agent Authentication powered by Streamlit, LangGraph, and Arcade AI✨ | by Plaban Nayak | The AI Forum | Medium
https://medium.com/the-ai-forum/building-building-a-next-generation-gmail-ai-assistant-with-agent-authentication-powered-by-01e9c8ae0f85
[2] [14] [15] [16] [17] [18] Building an AI Email Assistant That Prioritizes, Sorts, and Summarizes with LLMs - DEV Community
https://dev.to/malok/building-an-ai-email-assistant-that-prioritizes-sorts-and-summarizes-with-llms-34m8
[3] GitHub - blakeatech/blake-email-sender: Intelligent Gmail responder using fine-tuned OpenAI models, agentic processing, sentiment and intent detection, and semantic clustering.
https://github.com/blakeatech/blake-email-sender
[4] [10] [11] Building a Multi-Agent AI System: A Complete Guide to Secure Frontend-Backend Integration with CrewAI, LangChain, Exa and Groq | by Mudassar Hakim | Medium
https://medium.com/@mudassar.hakim/building-a-multi-agent-ai-system-a-complete-guide-to-secure-frontend-backend-integration-with-26a5a3aa0236
[5] Realtime | Supabase Docs
https://supabase.com/docs/guides/realtime
[7] Store and Structure Your Gmail Data with n8n and PostgreSQL (Part 3) | by Zafer KAHRAMAN | Medium
https://medium.com/@zafer_kahraman/build-a-gmail-to-postgresql-data-pipeline-with-n8n-part-3-366977ec8270
[8] Conversational Memory for LLMs with Langchain | Pinecone
https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/
[9] [12] [13] GitHub - parbhatkapila4/Vector-Mail
https://github.com/parbhatkapila4/Vector-Mail
